# -*- coding: utf-8 -*-
"""CIS545_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18CgzdyvHyO1ri6IYfAGydzaHr-nYD53n

## PREPROCESSING
The raw data for this project is energy data from the University of Pennsylvania. This data was gathered by hand by graduate students at the Center for Environmental Building and Design for projects, research, and reporting to FRES on energy consumption in their building stock. The data could only be accessed by downloading excel files containing a single day of energy data in 15 minute increments. Each daily file contains the consumption data for one of Electricity, Steam, or Chilled Water for all the buildings on campus that receive those energy carriers along with a timestamp. Not all buildings have steam or chilled water and some buildings that have electricity, steam or chilled water may be serviced from another building and thus may not be metered for one but still report on others.

These daily files were combined into monthly files, leaving in the headers for each daily segment and separated by an empty row. The preprocessing consists of taking this raw data and creating separate .csvs with the data for each building. Following that stage, the individual files are scanned for outliers, which are removed.
"""

from google.colab import drive
drive.mount('/content/drive')

"""The final stage of the preprocessing process is to take the outlier_removed data and to join it with weather data using the timestamp. The weather data used was purchased by the Center for Environmental Building + Design to be used with the energy data. It is, however, in hourly increments, so prior to joining the energy data must be resampled to hourly means.

Step 1: Import the necessary modules to pre-process the data
"""

import os, csv, datetime, matplotlib, copy
import numpy as np
import pandas as pd
import openpyxl as px
import matplotlib.dates as mdates
import matplotlib.pyplot as plt
import _pickle as pickle
from collections import defaultdict, OrderedDict
from dateutil import parser, rrule
from scipy import stats

"""Step 2: Access the raw data files from the google drive"""

from google.colab import drive
drive.mount('/content/gdrive')

"""Step 3: A few helper functions that will be useful for identifying the outlier data."""

def representFloat(self, string):
    try:
        float(string)
    except ValueError:
        return False
    else:
        return True

def getOutlierIdx(x,thresh=5.0):
    TorF = doubleMADsfromMedian(x,thresh)
    false_idx = [i for i in range(len(x)) if TorF[i] == True]       
    return false_idx

def doubleMADsfromMedian(y,thresh=2.8):
    b=len(y)
    nonzero_idx = np.where(y!=0)
    nonzeros = y[y != 0]
    m = np.median(nonzeros)
    abs_dev = np.abs(nonzeros - m)
    left_mad = np.median(abs_dev[nonzeros<=m])
    right_mad = np.median(abs_dev[nonzeros>=m])
    y_mad = np.zeros(len(nonzeros))
    y_mad[nonzeros < m] = left_mad
    y_mad[nonzeros > m] = right_mad
    modified_z_score = 0.6745 * abs_dev / y_mad
    modified_z_score[nonzeros == m] = 0
    d=np.zeros((len(y)),dtype=bool)
    d[nonzero_idx] = modified_z_score > thresh
    y=d 
    a=len(modified_z_score)
    print (a , b) 
    return y

"""Step 4: OmniEnergyPlot is a function that accepts a range of inputs to generate plots from the 15minute energy data."""

def omniEnergyPlot(plot_file_name, unit, df_list, **kwargs):
    style = ''
    if kwargs is not None:
        if 'style' in kwargs.keys():
            style = kwargs['style']
    if len(df_list) > 9:
        fig = plt.figure(figsize=(12, 8), dpi=300, facecolor='w', edgecolor='k')
    else:
        fig = plt.figure(figsize=(12, 2), dpi=300, facecolor='w', edgecolor='k')
    fig.text(0.5, 0.01, 'Time', ha='center', fontsize = 9)

    plot_dict = OrderedDict()
    color_list = ['k', 'r', 'b', 'y', 'g', 'm', 'c']
    for i in range(len(df_list)):
        plot_dict[i] = plt.subplot(1, 3, int(i+1))
        xy = df_list[i]
        x = xy.index
        if len(xy.shape) == 1:
            if style == 'no_mark':
                plot_dict[i].plot(x, xy, linewidth=0.5, color = color_list[0])
            elif style == 'histo':
                plot_dict[i].hist(xy, bins=50,  fc='k')
            elif style == 'bar':
                plot_dict[i].bar(x, xy.values)
            else:
                plot_dict[i].plot(x, xy, linewidth=0.5, linestyle = 'dotted', marker = 'x', markersize = 1, color = color_list[0]) 
        else:
            for j in range(xy.shape[1]):
                if j%2 == 0:
                    if style == 'scatter':
                        plot_dict[i].plot(x, xy[xy.columns[j]], color_list[j]+'x',label=(xy.columns)[j], markersize = 1) 
                    elif style == 'no_mark':
                        plot_dict[i].plot(x, xy[xy.columns[int(j)]], linewidth=1, label=(xy.columns)[int(j)], linestyle = 'dotted', color = color_list[int(j/2)])
                    elif style == 'bar':
                        plot_dict[i].bar(x, xy.values)
                    elif style == 'fill_between':
                        plot_dict[i].plot(x, xy[xy.columns[j]], linewidth=1, label=(xy.columns)[j], linestyle = 'dotted', marker = 'x', markersize = 1, color = color_list[int(j/2)])
                        if j != xy.shape[1]-1:
                            plot_dict[i].fill_between(x, xy[xy.columns[j]], xy[xy.columns[j+1]], where=xy[xy.columns[j]] > xy[xy.columns[j+1]], facecolor='green', interpolate=True)
                            plot_dict[i].fill_between(x, xy[xy.columns[j]], xy[xy.columns[j+1]], where=xy[xy.columns[j]] < xy[xy.columns[j+1]], facecolor='red', interpolate=True)
                    else:
                        plot_dict[i].plot(x, xy[xy.columns[j]], linewidth=1, label=(xy.columns)[j], linestyle = 'dotted', marker = 'x', markersize = 1, color = color_list[int(j/2)])    
                else:
                    if style == 'scatter':
                        plot_dict[i].plot(x, xy[xy.columns[j]], color_list[j-1]+'o', markersize = 4, label=(xy.columns)[j])
                    elif style == 'no_mark':
                        plot_dict[i].plot(x, xy[xy.columns[j]], linewidth=0.5, label=(xy.columns)[j], color = color_list[int((j+1)/2)-1])  
                    elif style == 'bar':
                        plot_dict[i].bar(x, xy)                          
                    elif style == 'fill_between':
                        print (i)
                        print (j)
                        print (xy.columns[j])
                        print (int(j+1)/2-1)
                        plot_dict[i].plot(x, xy[xy.columns[j]], linewidth=0.5, label=(xy.columns)[j], marker = 'o', markersize = 1, color = color_list[(int((j+1)/2)-1)])
                        if j != xy.shape[1]-1:
                            plot_dict[i].fill_between(x, xy[xy.columns[j]], xy[xy.columns[j+1]], where=xy[xy.columns[j]] > xy[xy.columns[j+1]], facecolor='green', interpolate=True)
                            plot_dict[i].fill_between(x, xy[xy.columns[j]], xy[xy.columns[j+1]], where=xy[xy.columns[j]] < xy[xy.columns[j+1]], facecolor='red', interpolate=True)
                    else:
                        plot_dict[i].plot(x, xy[xy.columns[j]], linewidth=0.5, label=(xy.columns)[j], marker = 'o', markersize = 1, color = color_list[int((j+1)/2)-1])                  
                if j == 0:
                    plt.ylabel(unit, multialignment='center', fontsize=7, labelpad=5)    
        plot_dict[i].autoscale(tight = True, axis = "x")
        plot_dict[i].yaxis.offsetText.set_fontsize(5)
        if len(df_list) == 1 and len(df_list[0].shape) == 1:
            plot_dict[i].legend(loc='upper center', ncol = 4, fontsize=6, bbox_to_anchor = (0.54,1.2))
        elif len(df_list) == 1 and df_list[0].shape[1]>5:
            plot_dict[i].legend(loc='upper center', ncol = 4, fontsize=6, bbox_to_anchor = (0.54,1.3))
        else: 
            plot_dict[i].legend(loc='upper center', ncol = 2, fontsize=6, bbox_to_anchor = (0.54,1.15))
        if type(x[0]) == pd.Timestamp and style != 'histo':
            plot_dict[i].xaxis.set_major_formatter(mdates.DateFormatter('%m/%y'))
        plt.tick_params(axis='both', which='major', labelsize=5)
        plt.tick_params(axis='both', which='minor', labelsize=5)

    plt.subplots_adjust(hspace=0.1)
    if 'annot' in kwargs.keys():
        plt.annotate(kwargs['annot'], xy=(.12, .9), xycoords='figure fraction', horizontalalignment='left', verticalalignment='top', fontsize=5)
        df = df_list[0] 
        row_labels=np.unique(df.index.year.values)[:-1]
        table_vals = []
        for i in range(len(row_labels)):
            if i == 0:
                table_vals.append(['baseline'])
            else:
                sums = df.loc[(df.index >= '%s-07-01'  % (row_labels[i])) & (df.index <= '%s-06-30' % (row_labels[i]+1)), :].sum(axis =0).values
                change = (sums[0] - sums[1])/sums[1]
                table_vals.append(["%0.2f percent" % (change*100.0)])
        the_table = plt.table(cellText=table_vals,colWidths = [0.1],rowLabels=row_labels,loc='upper right', bbox=[1.06, 0.7, 0.14, 0.3]) # x-axis shift, y-axis shift, box width, box height
        the_table.auto_set_font_size(False)
        the_table.set_fontsize(5)
    plt.tight_layout()
    plt.savefig(plot_file_name+'.png',dpi=300, bbox_inches='tight')
    plt.close()

"""Step 5: The Preprocess class. This allows a user to create an instance of a preprocess object that accepts a range of years as an input. It holds state for folder locations and a calender based on the date range inputs. It also defines the methods that will carry out each of the preprocess stages."""

class Preprocess():
    def __init__(self, start_year, end_year):

        # Sets the parent folder path
        self.parent_folder = "/content/gdrive/MyDrive/PennEnergyData/"

        # Sets the chw, stm, and elec folder path by appending 'CHW\\', 'STM\\', or 'ELC\\' to parent folder path
        self.CHW_folder = self.parent_folder + 'CHW/'
        self.STM_folder = self.parent_folder + 'STM/'
        self.ELC_folder = self.parent_folder + 'ELC/'

        self.bld_raw_folder = self.parent_folder + 'Buildings/raw_data/'
        self.fig_raw_folder = self.parent_folder + 'Figures/raw_data/'
        self.bld_out_folder = self.parent_folder + 'Buildings/raw_data_outliers_removed/'
        self.fig_out_folder = self.parent_folder + 'Figures/raw_data_outliers_removed/'

        # Creates a list of month name abbreviations to look for
        self.month_name = ['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec']

        # Sets the start and end year ranges based on the arguments passed into the constructor
        self.start_year = start_year
        self.end_year = end_year+1

        # Creates the calendar based on the energy files found within the range set by the user
        self.getFullCalendar()

    def getFullCalendar(self):

        # Initialize empty arrays to hold the potential starting and ending years/months
        start_month = []; start_year = []; end_month = []; end_year = []

        # Create a list of the 3 folder paths for the categories of 15-min raw energy data by month
        oslist = [self.ELC_folder, self.CHW_folder, self.STM_folder]

        # Look in each of the 3 folders of 15-min raw energy data by month
        for folder in oslist:

            # Get an array of the file paths in each folder sorted by Year-Month
            files = self.importFiles(folder)

            # For every month in the list of month names
            for month in self.month_name:

                # If the current month is the month of the first sorted filename then add it to the start_month array
                if month in os.path.basename(files[0]).lower():
                    start_month.append(self.month_name.index(month)+1)

                # If the current month is the month of the last sorted filename then add it to the end_month array
                if month in os.path.basename(files[-1]).lower():
                    end_month.append(self.month_name.index(month)+1)
                    
            # For every year in the range indicated by the user in the constructor        
            for year in range(self.start_year, self.end_year):

                # If the current year is the year of the first sorted filename then add it to the start_year array
                if str(year) in os.path.basename(files[0]).lower():
                    start_year.append(year)

                # If the current year is the year of the last sorted filename then add it to the end_year array
                if str(year) in os.path.basename(files[-1]).lower():
                    end_year.append(year)

        # Generate a full calendar based on the range of the earliest and latest month-year of the sorted file list
        self.full_calendar = self.generateFullYearCalendar([min(start_year), min(start_month)], [max(end_year), max(end_month)])

    def representFloat(self, string):
        try:
            float(string)
        except ValueError:
            return False
        else:
            return True 

    def generateFullYearCalendar(self, start_time, end_time):

        # Initializes an empty array to hold full set of date-times (15-min increments)
        full_sched_data = []

        # Fill the array based on 15 min increments between start_time and end_time
        print (start_time,end_time)
        for dt in rrule.rrule(rrule.MINUTELY, interval = 15, dtstart = datetime.datetime(start_time[0],start_time[1],1,0), until = datetime.datetime(end_time[0],end_time[1]+1,1,0)):
            full_sched_data.append(dt)

        # Ensure array is in proper datetime format and return it
        full_sched_data = np.array(full_sched_data).astype(datetime.datetime)[:-1]
        return full_sched_data

    def importFiles(self, folder):

        # Sets the folder being examined the the one passed in by the argument
        os.chdir(folder)

        # Creates an array of the all the file paths of the .xlsx files found in the folder
        unsorted_files = [os.path.abspath(f) for f in os.listdir(folder) if f.endswith('.xlsx')]

        # Initializes an empty array for the sorted file names
        sorted_files = []

        # For every year in the range set by the user during in the constructor
        for year in range(self.start_year, self.end_year):

            # And for every month in the lest of month abbreviations in the constructor
            for month_name in self.month_name:

                # Look at every file name in the list of unsorted file paths
                for f in unsorted_files:

                    # If the file name has the 2 digit year (17, 18, 19 etc) and month currently indicated by the
                    # above nested for-loops then append the file path to the sorted list. For-loops ensure order.
                    if str(year)[-2:] in os.path.basename(f).lower() and month_name in  os.path.basename(f).lower():
                        sorted_files.append(f)
                        break

        # Returns the sorted list of file paths        
        return sorted_files

    def dumpRawData(self):

        ### Set the three folders to look for energy data
        oslist = [self.ELC_folder, self.CHW_folder, self.STM_folder]

        ### For each of the three folders identified...
        for folder in oslist:

            ### Get the array of buildings names for which data was found in that folder, even if only available some times
            title = self.getBuildingsTitle(folder)

            ### Create a new OrderedDict()
            data_dict = OrderedDict()

            ### For every building name in the list of buildings create an entry in the OrderedDict() using the building name as the key
            print ("Adding building titles to data_dict")
            for t in title:
                data_dict[t] = []

            ### Creates an array containing the paths for all the files in the folder (monthly 15-min increment, all buildings)
            files = self.importFiles(folder)

            ### For every filepath in the folder
            for f in files:
                print ("\nStart converting at file:", os.path.basename(f))

                ### Load the workbook from the file and get the names of all the sheets. Create an empty array for data.
                W = px.load_workbook(f, read_only = True, data_only = True)
                ws = W.get_sheet_names()
                data = []

                ### Obtain the correct sheet containting data. If Combine or Sheet0 exists it will be in those preferentially, only in Sheet1 if they dont exist
                if 'Combine Sheet' in ws:
                    p = W.get_sheet_by_name(name = 'Combine Sheet')
                elif 'Sheet0' in ws:
                    p = W.get_sheet_by_name(name = 'Sheet0')
                else:
                    p = W.get_sheet_by_name(name = 'Sheet1')

                ### Initialize index_title array and iterate through the rows in the sheet identified
                index_title = ['Time', 'Total Use']
                for row in p.iter_rows():
                    
                    ### Find the rows that have data in column A (0) and nothing in column B (1), they are they ones with the building names
                    if row[0].value and row[1].value == None:
     
                        ### Chop off the first column in the row and look at each cell remaining
                        row = row[1:]
                        for cell in row:

                            ### If the cell has a value and that trimmed value is not already in the index_title array then add it to the array
                            if ((cell.value is not None) and (str(cell.value).replace(' (kW)', '').replace(' (kBTU/h)','').replace(' (BTU/h)', '').replace('/', '-') not in index_title)):
                                index_title.append(str(cell.value).replace(' (kW)', '').replace(' (kBTU/h)','').replace(' (BTU/h)', '').replace('/', '-')) 

                ### Iterate through the rows again and identify the rows thathave values in columns A and B (row[0] and row[1])
                for row in p.iter_rows():                    
                    if row[0].value and row[1].value:

                        ### Determine the data type of row[1] to see if it is full date-time or just time
                        ### If its just time then the value array can set its row[0] and row[1] to equal those of the data row and then row[2+] equal to the
                        ### energy meter readings. If it is date-time, then extract the time and use that for row[1] of the value array, otherwise as same.
                        ### The value array is added to the data array.
                        if type(row[1].value) == datetime.time:                          
                            value = [datetime.datetime.combine(row[0].value, row[1].value)]+\
                                    [cell.value for cell in row if cell.value is not None][2:]
                            data.append(value)
                        elif type(row[1].value) == datetime.datetime:
                            t = row[1].value.time()
                            value = [datetime.datetime.combine(row[0].value, t)]+\
                                [cell.value for cell in row if cell.value is not None][2:]
                            data.append(value)

                ### Looking at all the building name keys added to the data_dict
                for t in data_dict.keys():
                    
                    ### If the building name key is found in this month of data it will be in the index_title
                    if t in index_title:

                        ### Set the index position for data to equal the index of the building name in the file
                        idx = index_title.index(t)

                        ### Looks at all the value arrays in data and appends the value corrensponding to the index for the building to the data_dict
                        ### keyed by thst building name
                        print ("Adding data for: "+t)
                        data_dict[t]+=[d[idx] for d in data]

                    ### If the building name from the data_dict keys wasn't in the files index of buildings, then fill in a number of values '-' equal to length of data
                    else:
                        print ('missing:', t,",interpolating with zeros.")
                        data_dict[t]+=['-']*len(data)

                ### Prints the size of the 2-d array (columns and rows) of data found in the data_dict (all values for a month)
                print (np.array(list(data_dict.values())).transpose().shape)

            ### Creates a new array holding the transposed values from the data_dict for all the data                    
            data_arr = np.array(list(data_dict.values())).transpose()

            ### If folder is CHW converts CHW units to kBtu based on date ranges
            if folder == self.CHW_folder:
                for arr in data_arr:
                    if (arr[0].year == 2015):
                        if (arr[0].month >= 7):
                            for i in range(1, len(arr)):
                                if self.representFloat(arr[i]):
                                    arr[i] = arr[i] / 1000
                    elif (arr[0].year == 2016):
                        if (arr[0].month < 5):
                            for i in range(1, len(arr)):
                                if self.representFloat(arr[i]):
                                    arr[i] = arr[i] / 1000

            ### If folder is ELC converts ELC units to kBtu based on date ranges
            if folder == self.ELC_folder:
                for arr in data_arr:
                    # converting units for data with different units
                    if (arr[0].year == 2014):
                        for i in range(1, len(arr)):
                            if self.representFloat(arr[i]):
                                arr[i] = arr[i] / 0.293071039
                    elif (arr[0].year == 2015):
                        for i in range(1, len(arr)):
                            if self.representFloat(arr[i]):
                                arr[i] = arr[i] / 0.293071039
                    elif (arr[0].year == 2016):
                        if (arr[0].month == 8):
                            for i in range(1, len(arr)):
                                if self.representFloat(arr[i]):
                                    arr[i] = arr[i] / 1000
                        else: 
                            for i in range(1, len(arr)):
                                if self.representFloat(arr[i]):
                                    arr[i] = arr[i] / 0.293071039
                    elif (arr[0].year == 2017):
                        if (arr[0].month < 5):
                            for i in range(1, len(arr)):
                                if self.representFloat(arr[i]):
                                    arr[i] = arr[i] / 0.293071039
                        elif (arr[0].month >=7):
                            for i in range(1, len(arr)):
                                if self.representFloat(arr[i]):
                                    arr[i] = arr[i] / 1000
                    elif (arr[0].year == 2018):
                        if (arr[0].month < 7):
                            for i in range(1, len(arr)):
                                if self.representFloat(arr[i]):
                                    arr[i] = arr[i] / 1000

            ### If folder is STM converts STM units to kBtu based on date ranges
            elif folder == self.STM_folder:
                for arr in data_arr:
                    # converting units for data with different units
                    if (arr[0].year == 2015):
                        if (arr[0].month >= 7):
                            for i in range(1, len(arr)):
                                if self.representFloat(arr[i]):
                                    arr[i] = arr[i] / 0.293071039
                    elif (arr[0].year == 2016):
                        if (arr[0].month < 5):
                            for i in range(1, len(arr)):
                                if self.representFloat(arr[i]):
                                    arr[i] = arr[i] / 0.293071039
                        if (arr[0].month == 8):
                            for i in range(1, len(arr)):
                                if self.representFloat(arr[i]):
                                    arr[i] = arr[i] / 3412

            ### Opens and write the titles and data to the all.csv file
            with open(folder+'all.csv', 'w') as w:
                w = csv.writer(w)
                w.writerow(title)
                w.writerows(data_arr)
                print ("Done Dumping Raw Data")

    def getBuildingsTitle(self, folder):

        ### Get sorted array of strings  of pathnames for each file in folder, initialize two empty sets, and start array of building/column names
        files = self.importFiles(folder)
        hash_list = set()
        buildings = set()
        title = (['Time', 'Total Use'])

        ### For each file pointed to by a filepath
        for f in files:

            ### Print the filename in the filepath
            print (os.path.basename(f).lower())

            ### Loads the workbook from the 15-minute increment monthly data for all buildings .csv file 
            W = px.load_workbook(f, read_only = True, data_only = True)

            ### Get the names of the worksheets in the workbook
            ws = W.get_sheet_names()

            ### Looks for the 3 possible sheetnames for the relevant data, if it finds one then it sets p to be that sheet,
            ### there will only be one with relevant data but the order it searches ensures that the proper one is returned
            if 'Combine Sheet' in ws:
                p = W.get_sheet_by_name(name = 'Combine Sheet')
            elif 'Sheet0' in ws:
                p = W.get_sheet_by_name(name = 'Sheet0')
            elif 'Sheet1' in ws:
                p = W.get_sheet_by_name(name = 'Sheet1')

            ### Look at every row in the the worksheet of 15 minute data for one month for all buildings
            for row in p.iter_rows():

                #### If columns 1 and 2 of the row have no value               
                if row[1].value == None and row[2].value == None:

                    ### Remove the first three cells (date and two empty)
                    row = row[3:]

                    ### Hash the row and see if the hashcode is already in the hashList (ie that exact row has been seen before
                    ### and can thus be ignored. If not...
                    row_hash = hash(str(row))
                    if row_hash not in hash_list:

                        ### First add the hashcode to the hashList set
                        hash_list.add(row_hash)

                        ### Then look at every cell in the row and if its value is not None...
                        for cell in row:
                            if (cell.value is not None and str(cell.value) != " "):

                                ### Trim the units from the cell value to get the building name
                                building = str(cell.value).replace(' (kW)', '').replace(' (kBTU/h)','').replace(' (BTU/h)', '').replace('/', '-')
                                
                                ### If that building is not in the buildings set then add it to the set and append it to title
                                if building not in buildings:
                                    buildings.add(building)
                                    title.append(building)
                                    print ("Added and appended: " + building)        
        return title

    def getRawDataDict(self):
        print ("Getting raw data dictionary")
        ### Get a blank OrderedDict() and create an array of the category folders and an array of the category names
        data_dict = OrderedDict()
        oslist = [self.ELC_folder, self.CHW_folder, self.STM_folder]
        cat_list = ['ELC', 'CHW', 'STM']

        ### Iterate through 1-3
        for i in range(len(oslist)):

            ### Read the all.csv file in the folder indicated by i and get the dataframe
            df_cat = pd.read_csv(oslist[i] + 'all.csv')

            ### Convert the time column to date-time and then make that the index for the dataframe
            df_cat['Time'] = pd.to_datetime(df_cat['Time'])
            df_cat = df_cat.set_index('Time')

            ### Order it according the date-time column
            df_cat = df_cat.groupby(df_cat.index).first()

            ### Converts all values to numeric, converting non-numeric values to NaN
            print ('coercing to NaN...')
            df_cat = df_cat.apply(pd.to_numeric, errors = 'coerce')

            ### Takes the raw data and re-indexes it to a full calendar, filling in any missing dates (missing for all buildings) with NaN
            df_cat = df_cat.reindex(self.full_calendar)

            ### Saves the filled dataframe for the entire category to the data_dict
            data_dict[cat_list[i]] = df_cat

        ### Returns the data_dictionary after all three categories have been added                       
        return data_dict

    ###################################################################################################################
    ### Creates the 15-minute raw data files and plots charts for each building                                     ###
    ###################################################################################################################
    def dumpRawData_building(self):

        ### Sets relevant folder paths to export data and figures for raw data for each building 
        target_folder_csv = self.bld_raw_folder
        target_folder_figure = self.fig_raw_folder

        ### If either folder does not exist then create it in the appropriate locations within the parent dir
        if not os.path.exists(target_folder_figure):
            os.makedirs(target_folder_figure)  
        if not os.path.exists(target_folder_csv):
            os.makedirs(target_folder_csv)

        ### Obtains the full data dictionary for the raw data, with missing values and date coerced to NaN
        data_dict = self.getRawDataDict()

        ### Initialize a blank new_dict and an array with the Category names
        new_dict = {}
        cat_list = ['ELC', 'CHW', 'STM']

        ### Iterate through each of the categories identified in the cat_list array
        for cat in cat_list:

            ### For every building name found in the raw data columns
            for building in data_dict[cat].columns:

                ### If the building name is not one of the keys in the new_dict then add it as a key with no data
                if building not in new_dict.keys():
                    new_dict[building] = {}

                ### Add the data for that building and category but reverse the keys so the data is gouped by building as the primary key
                new_dict[building][cat] = data_dict[cat][building].to_frame()

        ### For every building in the new_dict (which now matches those in the data_dict)
        for building in new_dict.keys():

            ### Now iterate through the defined categories
            for cat in cat_list:

                ### If the building in new_dict does not have data for that category, create a key for it and fill with NaN
                if cat not in new_dict[building].keys():
                    new_dict[building][cat] = pd.Series(np.nan, index = self.full_calendar).to_frame()

                ### Check and see if the building in new_dict has a 'merged' key, if not create one and set it equal to the current category dataframe
                if 'merged' not in new_dict[building].keys():
                    new_dict[building]['merged'] = new_dict[building][cat]

                ### If it already has a 'merged' key, then merge the dataframe for the current cat into the 'merged' key.
                ### This combines all three categories for the building into a single dataframe with one date-time index
                else:
                    new_dict[building]['merged'] = pd.merge(new_dict[building]['merged'], new_dict[building][cat], how = 'outer', left_index = True, right_index = True)

            ### Set the column names for 'merged' category fr this building to equal the category list
            new_dict[building]['merged'].columns = cat_list

            ### Replace all the data for the building with just the 'merged' dataframe (eliminates individual categories)
            new_dict[building] = new_dict[building]['merged']

            ### Rename the index column as 'Time'
            new_dict[building].index.names = ['Time']

            ### Print the raw, merged category data for that building to 'building_name'.csv in the raw_data folder
            new_dict[building].to_csv(target_folder_csv + building + '.csv')
            print (building + ".csv raw data saved")

            ### Plot the raw data and save it to the appropriate folder
            omniEnergyPlot(target_folder_figure+building, 'kWh', [new_dict[building]['ELC'].fillna(value = 0.0), new_dict[building]['CHW'].fillna(value = 0.0),
                new_dict[building]['STM'].fillna(value = 0.0)], style = 'no_mark')

        ### Save and return the raw data dictinary
        pickle.dump(new_dict, open(self.parent_folder + 'raw_data_dict.save', 'wb'), protocol=-1)
        return new_dict  

    def getWeather(self):
        df = pd.read_csv(self.parent_folder+"Weather/weather.csv")[['time_valid_lcl', 'temperature_air_2m_f', 'humidity_relative_2m_pct', 'pressure_2m_mb', 'wind_speed_10m_kts', 'precipitation_in', 'cloud_cover_pct', 'radiation_solar_total_wpm2', 'snowfall_estimated_in']]
        df.columns = ['datetime', 'temperature', 'rh', 'pressure', 'wind speed', 'precipitation', 'cloud cover', 'irradiation', 'snow fall']
        df.set_index('datetime', inplace = True)
        df.index = pd.to_datetime(df.index)
        df[['temperature']] = ((df[['temperature']] - 32)*(5.0/9)).round(2)
        df = df.apply(pd.to_numeric, args=('coerce',))
        return df 

    ##################################################################################################################
    ### Creates the 15-minute raw data files with outlier removed for each building                                 ###
    ###################################################################################################################
    def dumpOutlierRemoved_building(self):
        print ("Dumping outliers...")
        
        ### Retrieve the save raw data dictionary generated by the dump_raw_data function
        dct = pickle.load(open(self.parent_folder + 'raw_data_dict.save', 'rb'))

        ### Set the target folders for saving the outlier free data and the charts for it. If they don't already exist, create them
        target_folder_csv = self.bld_out_folder
        target_folder_figure = self.fig_out_folder
        if not os.path.exists(target_folder_figure):
            os.makedirs(target_folder_figure)  
        if not os.path.exists(target_folder_csv):
            os.makedirs(target_folder_csv) 

        ### Get weather data
        wea_data = self.getWeather()

        ### Create arrays containing the years, months, days, and hours for all the index values
        month = np.array([d.month for d in wea_data.index])
        hour = np.array([d.hour for d in wea_data.index])

        ### Add the Month and Hour columns to the weather dataframe
        df_wea = pd.DataFrame({'month':month, 'hour': hour}, index = wea_data.index)

        ### Remove the Pressure column from the weather dataframe
        df_wea = df_wea.join(wea_data.drop('pressure', axis = 1))

        ### Label the weather dataframe columns
        df_wea.columns = ['month', 'hour', 'temp', 'rh', 'wind', 'precipitation', 'cloud', 'irr', 'snow']

        ### Orders the weather dataframe according to the DateTime index
        df_wea = df_wea.groupby(df_wea.index).first()

        ### Removes all rows or columns that contain NA / Null values and returns the means for the hours by thetime index
        df_wea = df_wea.resample('H').mean().dropna(how = 'any')

        ### For every building set as a key in the raw_data_dict...
        for building in dct.keys():

            ### Relace all NaN and - with 0.0
            dct[building] = dct[building].fillna(value = 0.0)
            print ('\n',building)

            ### For each column in the building (ELC, STM, CHW)
            for cat in dct[building].columns:

                ### Grab the data values for that category
                values = dct[building][cat].values

                ### Create a new dataframe from that building/category
                df = dct[building][cat].to_frame()

                ### Removes date-time as index and returns it as a column, new numbered index replaces them,order is retained.
                df.reset_index(inplace=True)

                ### Returns a list of the index positions of the data points identified as outliers in the data, print the category and the number of outliers removed
                outlier_idx = getOutlierIdx(values)
                print ("Outliers removed of Total: " + cat + ", " + str(len(outlier_idx))+ ", " + str(len(values)))

                ### Set the identified outlier in that category to 0.0 in the dataframe and return the index to be the Time column
                df.loc[outlier_idx, cat] = 0.0
                df = df.set_index('Time')

                ### Save the outlier removed dataframe back into the dictionatry
                dct[building][cat] = df.values

            ### Write the data dictionary for that building to .csv and plot the outlier removed data
            dct[building].to_csv(target_folder_csv + building + '.csv')
            omniEnergyPlot(target_folder_figure+building, 'kWh', [dct[building]['ELC'], dct[building]['CHW'], dct[building]['STM']], style = 'no_mark')

            ### Resample the 15-minute energy data into hourly means
            print (building)
            if dct[building].empty:
                print('No info for:' + building)
                continue
            dct[building] = dct[building].resample('H').mean()
            dct[building].to_csv(target_folder_csv + building + '_hourly.csv')

            ### Join weather data to energy data
            dct[building] = pd.concat([df_wea[['month', 'hour', 'temp', 'rh', 'wind', 'precipitation', 'cloud', 'irr', 'snow']], dct[building]], axis=1, join='inner').dropna(how='any')            
            dct[building].to_csv(target_folder_csv + building + '_hourly_weather.csv')

        ### Save the data_no_outliers data dictionary and then return it
        pickle.dump(dct, open(self.parent_folder + 'data_no_outliers.save', 'wb'), protocol=-1) 
        return dct

"""This is the intial step that parses all the individual monthly files and generates the individual building .csvs"""

prepro = Preprocess(2015, 2019)

prepro.dumpRawData()

"""The next step is to generate individual csvs for each building merging the three energy types into different columns of the same file"""

prepro = Preprocess(2015, 2019)
prepro.dumpRawData_building()

"""In the final preprocessing step, the outliers are identified and removed from each raw data file. These results are then aggregated to be in hourly rather than 15min increments and then each joined with the weather data to create a data dict for each building that can be split to create training, validation, and testing sets for ML."""

!pip3 install pickle5
import pickle5 as pickle
prepro.dumpOutlierRemoved_building()

"""## EDA: Exploratory Data Analysis

Now that the data has been preprocessed, some exploration of the data can take place. This section will examine both the energy data and the weather data individually, as well as combined.

The energy data consists of over 100 buildings and even after preprocessing, not all of the buildings are suitable. The primary reason encountered is missing data. The meters at Penn are imperfect and, especially in early years, could be missing weeks or months of data at a time. Further, the naming convention of the meters changes over time, as buildings are renamed or repurposed leaving some meters missing years of data. In some instances, one meter of the three may be identified by a different building name than the others, so that the data was not joined. Finally, in some cases where one building recieves its energy through a parent building, the child buildings energy was not metered until part way through the timespan, causeing a sudden drop when it is removed. Because of this we have chosen to identify 10 buildings with excellent data quality to develop our models.

Charts were generated during the preprocess stages that show the energy data for each building as a line chart, both with and without outliers. This was used to evaluate the effectiveness of the outlier removal protocols as well as to gauge the overall quality of the data resulting.

Below we see charts for 3 buildings. The first column has electric data, the second has chilled water data, and the thrd has steam data. The first row shows energy data for a building with high quality raw data, even before preprocessing. The second row shows an example of a building dominated by a few outliers. And the final row shows a building with significant missing data.
"""

from IPython.display import Image, display
display(Image('/content/gdrive/MyDrive/PennEnergyData/Figures/raw_data/Evans.png'))
display(Image('/content/gdrive/MyDrive/PennEnergyData/Figures/raw_data/Class of 1920 Dining Commons.png'))
display(Image('/content/gdrive/MyDrive/PennEnergyData/Figures/raw_data/Fels.png'))

"""The charts below show the same 3 buildings after the outliers have been removed. As can be seen, the second building looks much better after outlier removal, while the third building is still missing too much data to be useful, although it is improved."""

display(Image('/content/gdrive/MyDrive/PennEnergyData/Figures/raw_data_outliers_removed/Evans.png'))
display(Image('/content/gdrive/MyDrive/PennEnergyData/Figures/raw_data_outliers_removed/Class of 1920 Dining Commons.png'))
display(Image('/content/gdrive/MyDrive/PennEnergyData/Figures/raw_data_outliers_removed/Fels.png'))

def displayEnergySummary(file):
  building_year_month_df, building_year_df, building_month_df = energyDataSummary(file)

  fig, axes = plt.subplots(nrows=1, ncols=3)
  fig.set_size_inches(15,2)
  building_year_month_df[['max_elc', 'ave_elc', 'min_elc']].plot(ax=axes[0])
  building_year_month_df[['max_chw', 'ave_chw', 'min_chw']].plot(ax=axes[1])
  building_year_month_df[['max_stm', 'ave_stm', 'min_stm']].plot(ax=axes[2])

  fig, axes = plt.subplots(nrows=1, ncols=3)
  fig.set_size_inches(15,2)
  building_month_df[['max_elc', 'ave_elc', 'min_elc']].plot(ax=axes[0])
  building_month_df[['max_chw', 'ave_chw', 'min_chw']].plot(ax=axes[1])
  building_month_df[['max_stm', 'ave_stm', 'min_stm']].plot(ax=axes[2])

  fig, axes = plt.subplots(nrows=1, ncols=3)
  fig.set_size_inches(15,2)
  building_year_df[['max_elc', 'ave_elc', 'min_elc']].plot(ax=axes[0])
  building_year_df[['max_chw', 'ave_chw', 'min_chw']].plot(ax=axes[1])
  building_year_df[['max_stm', 'ave_stm', 'min_stm']].plot(ax=axes[2])

def joinEnergySummary(df, joinon):
  print(df)
  mean_df = df[[joinon, 'ELC', 'CHW', 'STM']].groupby(joinon).mean()
  mean_df.columns = ['ave_elc', 'ave_chw', 'ave_stm']
  max_df = df[[joinon, 'ELC', 'CHW', 'STM']].groupby(joinon).max()
  max_df.columns = ['max_elc', 'max_chw', 'max_stm']
  min_df = df[[joinon, 'ELC', 'CHW', 'STM']].groupby(joinon).min()
  min_df.columns = ['min_elc', 'min_chw', 'min_stm']
  ret_df = min_df.merge(mean_df, on=joinon, how='inner').merge(max_df, on=joinon, how='inner')
  return ret_df

def energyDataSummary(file):
  building_df = pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Buildings/raw_data_outliers_removed/'+file)
  building_df['Time'].astype('datetime64')
  building_df['date'] = building_df['Time'].map(lambda x: x.split(' ')[0])
  building_df['year_month'] = building_df['Time'].map(lambda x: x.split('-')[0]+' '+x.split('-')[1])
  building_df['year'] = building_df['Time'].map(lambda x: x.split('-')[0])
  building_df['month'] = building_df['Time'].map(lambda x: x.split('-')[1])

  building_year_month_df = joinEnergySummary(building_df, 'year_month')
  building_year_df = joinEnergySummary(building_df, 'year')
  building_month_df = joinEnergySummary(building_df, 'month')
  
  return building_year_month_df, building_year_df, building_month_df

"""The following shows the outlier removed energy charts for the 10 buildings that were chosen. They are:

1.    The Annenberg Center
"""

display(Image('/content/gdrive/MyDrive/PennEnergyData/Figures/raw_data_outliers_removed/Annenberg Center.png'))
displayEnergySummary('Annenberg Center.csv')

"""2.    Blockley Hall"""

display(Image('/content/gdrive/MyDrive/PennEnergyData/Figures/raw_data_outliers_removed/Blockley Hall.png'))
displayEnergySummary('Blockley Hall.csv')

"""3.    BRB1-Stellar Chance"""

display(Image('/content/gdrive/MyDrive/PennEnergyData/Figures/raw_data_outliers_removed/BRB I Stellar Chance.png'))
displayEnergySummary('BRB I Stellar Chance.csv')

"""4.    Charles Addams Hall"""

display(Image('/content/gdrive/MyDrive/PennEnergyData/Figures/raw_data_outliers_removed/Charles Addams.png'))
displayEnergySummary('Charles Addams.csv')

"""5.    Fisher Fine Arts and Duhring Wing"""

display(Image('/content/gdrive/MyDrive/PennEnergyData/Figures/raw_data_outliers_removed/Fisher and Duhring Wing.png'))
displayEnergySummary('Fisher and Duhring Wing.csv')

"""6.    Grad Towers B- Sansom West"""

display(Image('/content/gdrive/MyDrive/PennEnergyData/Figures/raw_data_outliers_removed/Grad Towers B (West).png'))
displayEnergySummary('Grad Towers B (West).csv')

"""7.    Johnson Pavilion"""

display(Image('/content/gdrive/MyDrive/PennEnergyData/Figures/raw_data_outliers_removed/Johnson Pavillion.png'))
displayEnergySummary('Johnson Pavillion.csv')

"""8.    The Penn Museum"""

display(Image('/content/gdrive/MyDrive/PennEnergyData/Figures/raw_data_outliers_removed/Museum.png'))
displayEnergySummary('Museum.csv')

"""9.    Singh Nanotechnology"""

display(Image('/content/gdrive/MyDrive/PennEnergyData/Figures/raw_data_outliers_removed/Singh Nano Bldg.png'))
displayEnergySummary('Singh Nano Bldg.csv')

"""10.   Vagelos Labs"""

display(Image('/content/gdrive/MyDrive/PennEnergyData/Figures/raw_data_outliers_removed/Vagelos Labs (IAST).png'))
displayEnergySummary('Vagelos Labs (IAST).csv')

"""Now lets explore the weather data a little bit"""

def weatherDataSummary():
  building_df = pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Weather/weather.csv')
  building_df['time_valid_lcl'].astype('datetime64')
  building_df['date'] = building_df['time_valid_lcl'].map(lambda x: x.split(' ')[0])
  building_df['year_month'] = building_df['date'].map(lambda x: x.split('/')[2]+' '+x.split('/')[0])
  building_df['year'] = building_df['date'].map(lambda x: x.split('/')[2])
  building_df['month'] = building_df['date'].map(lambda x: x.split('/')[0])

  building_year_month_df = joinWeatherSummary(building_df, 'year_month')
  building_year_df = joinWeatherSummary(building_df, 'year')
  building_month_df = joinWeatherSummary(building_df, 'month')
  
  return building_year_month_df, building_year_df, building_month_df

def joinWeatherSummary(df, joinon):
  print(df)
  mean_df = df[[joinon, 'temperature_air_2m_f', 'humidity_relative_2m_pct', 'wind_speed_10m_kts', 'precipitation_in']].groupby(joinon).mean()
  mean_df.columns = ['ave_temp', 'ave_rh', 'ave_wind', 'ave_rain']
  max_df = df[[joinon, 'temperature_air_2m_f', 'humidity_relative_2m_pct', 'wind_speed_10m_kts', 'precipitation_in']].groupby(joinon).max()
  max_df.columns = ['max_temp', 'max_rh', 'max_wind', 'max_rain']
  min_df = df[[joinon, 'temperature_air_2m_f', 'humidity_relative_2m_pct', 'wind_speed_10m_kts', 'precipitation_in']].groupby(joinon).min()
  min_df.columns = ['min_temp', 'min_rh', 'min_wind', 'min_rain']
  ret_df = min_df.merge(mean_df, on=joinon, how='inner').merge(max_df, on=joinon, how='inner')
  return ret_df

def displayWeatherSummary():
  weather_year_month_df, weather_year_df, weather_month_df = weatherDataSummary()

  fig, axes = plt.subplots(nrows=1, ncols=4)
  fig.set_size_inches(15,2)
  weather_year_month_df[['max_temp', 'ave_temp', 'min_temp']].plot(ax=axes[0])
  weather_year_month_df[['max_rh', 'ave_rh', 'min_rh']].plot(ax=axes[1])
  weather_year_month_df[['max_wind', 'ave_wind', 'min_wind']].plot(ax=axes[2])
  weather_year_month_df[['max_rain', 'ave_rain', 'min_rain']].plot(ax=axes[3])

  fig, axes = plt.subplots(nrows=1, ncols=4)
  fig.set_size_inches(15,2)
  weather_month_df[['max_temp', 'ave_temp', 'min_temp']].plot(ax=axes[0])
  weather_month_df[['max_rh', 'ave_rh', 'min_rh']].plot(ax=axes[1])
  weather_month_df[['max_wind', 'ave_wind', 'min_wind']].plot(ax=axes[2])
  weather_month_df[['max_rain', 'ave_rain', 'min_rain']].plot(ax=axes[3])

  fig, axes = plt.subplots(nrows=1, ncols=4)
  fig.set_size_inches(15,2)
  weather_year_df[['max_temp', 'ave_temp', 'min_temp']].plot(ax=axes[0])
  weather_year_df[['max_rh', 'ave_rh', 'min_rh']].plot(ax=axes[1])
  weather_year_df[['max_wind', 'ave_wind', 'min_wind']].plot(ax=axes[2])
  weather_year_df[['max_rain', 'ave_rain', 'min_rain']].plot(ax=axes[3])

displayWeatherSummary()

"""#On to Machine Learning!

### Our goal
First, we have to figure out what we want to do with this data in the first place. Looking at our training data, we can see there are features for the month and hour of the given date, the temperature and other weather data, and then electricity, chilled water, and steam energy outputs. Thus, we know we need to use the weather data to predict one of the three energy outputs.

We will run 60 models total for this project -- 6 for each of the 10 buildings we decided to work on to cover the 3 outputs and the two types of models we want to run.

### Accounting for seasonality
As you can see from the plots above, the data is pretty seasonal, especially temperature. This makes sense, considering weather data is maybe the most seasonal thing out there (at least until global warming changes that). The first step in model-building is to account for that seasonality.
"""

#According to the class slides, I should remove an exponential weighted moving average to eliminate seasonality through differencing 
def remove_seasonality(df):
 result = df.copy()
 result[['temp','rh', 'wind', 'precipitation', 'cloud', 'irr', 'snow', 'ELC', 'CHW', 'STM']] = result[['temp','rh', 'wind', 'precipitation', 'cloud', 'irr', 'snow', 'ELC', 'CHW', 'STM']] - result[['temp','rh', 'wind', 'precipitation', 'cloud', 'irr', 'snow', 'ELC', 'CHW', 'STM']].ewm(com = .5).mean()
 return result

"""Here's what the temperature data looked like before deseasonalizing"""

pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Annenberg Center_hourly_weather.csv')['temp'].plot()

"""Here's what the temperature data looks like after deseasonalizing:"""

annenberg_df = remove_seasonality(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Annenberg Center_hourly_weather.csv'))
blockley_df = remove_seasonality(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Blockley Hall_hourly_weather.csv'))
brb1_df = remove_seasonality(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/BRB I Stellar Chance_hourly_weather.csv'))
addams_df = remove_seasonality(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Charles Addams_hourly_weather.csv'))
fisherfine_df = remove_seasonality(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Fisher and Duhring Wing_hourly_weather.csv'))
sansomwest_df = remove_seasonality(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Grad Towers B (West)_hourly_weather.csv'))
johnsonpavilion_df = remove_seasonality(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Johnson Pavilion_hourly_weather.csv'))
museum_df = remove_seasonality(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Museum_hourly_weather.csv'))
singh_df = remove_seasonality(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Singh Nano Bldg_hourly_weather.csv'))
vagelos_df = remove_seasonality(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Vagelos Labs (IAST)_hourly_weather.csv'))
annenberg_df['temp'].plot()

"""### Normalizing and scaling the data
First, we need to scale our data. We will need to use PCA (Primary Component Analysis) to select features, and we know that PCA is not scale-invariant. As a result, our features need to all be in the same range in order for PCA to tell us anything meaningful about which variables to select. This cell makes a scaler that will transform everything in the data to a 0-1 scale, where 0 is the min value and 1 is the max value.
Note that we don't scale the date columns, since that is not necessary.
"""

# pretty common min-max scaler from sklearn
# source: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
def normalize(df):
    result = df.copy()
    result[['temp','rh', 'wind', 'precipitation', 'cloud', 'irr', 'snow', 'ELC', 'CHW', 'STM']] = scaler.fit_transform(result[['temp','rh', 'wind', 'precipitation', 'cloud', 'irr', 'snow', 'ELC', 'CHW', 'STM']])
    return result

"""Normalizing for each building:"""

annenberg_normalized = normalize(annenberg_df)
blockley_normalized = normalize(blockley_df)
brb1_normalized = normalize(brb1_df)
addams_normalized = normalize(addams_df)
fisherfine_normalized = normalize(fisherfine_df)
sansomwest_normalized = normalize(sansomwest_df)
johnsonpavilion_normalized = normalize(johnsonpavilion_df)
museum_normalized = normalize(museum_df)
singh_normalized = normalize(singh_df)
vagelos_normalized = normalize(vagelos_df)

"""Extra code (ignore)"""

# annenberg_normalized = normalize(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Annenberg Center_hourly_weather.csv'))
# blockley_normalized = normalize(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Blockley Hall_hourly_weather.csv'))
# brb1_normalized = normalize(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/BRB I Stellar Chance_hourly_weather.csv'))
# addams_normalized = normalize(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Charles Addams_hourly_weather.csv'))
# fisherfine_normalized = normalize(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Fisher and Duhring Wing_hourly_weather.csv'))
# sansomwest_normalized = normalize(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Grad Towers B (West)_hourly_weather.csv'))
# johnsonpavilion_normalized = normalize(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Johnson Pavilion_hourly_weather.csv'))
# museum_normalized = normalize(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Museum_hourly_weather.csv'))
# singh_normalized = normalize(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Singh Nano Bldg_hourly_weather.csv'))
# vagelos_normalized = normalize(pd.read_csv('/content/gdrive/MyDrive/PennEnergyData/Training Data/Vagelos Labs (IAST)_hourly_weather.csv'))
# display(annenberg_normalized)

"""### Regression Model
First, let's try to model our outputs using a multiple regression model. To be honest, it's unlikely that this model will be very accurate, but it's still a great baseline anyways. We'll make a function that will run a linear model for all 3 outputs and print the summaries for each.

First, we'll split our data into testing and training data before calling the function:

"""

from sklearn.model_selection import train_test_split
def split_data(passedDF):
  result = passedDF.copy()
  result_x = result[['temp','rh', 'wind', 'precipitation', 'cloud', 'irr', 'snow']]
  result_y = result[['ELC', 'CHW', 'STM']]
  result_x_train, result_x_test, result_y_train, result_y_test = train_test_split(result_x, result_y, test_size = .3, random_state = 23)
  return result_x_train, result_x_test, result_y_train, result_y_test

annenberg_x_train, annenberg_x_test, annenberg_y_train, annenberg_y_test = split_data(annenberg_normalized)
blockley_x_train, blockley_x_test, blockley_y_train, blockley_y_test = split_data(blockley_normalized)
brb1_x_train, brb1_x_test, brb1_y_train, brb1_y_test = split_data(brb1_normalized)
addams_x_train, addams_x_test, addams_y_train, addams_y_test = split_data(addams_normalized)
fisherfine_x_train, fisherfine_x_test, fisherfine_y_train, fisherfine_y_test = split_data(fisherfine_normalized)
sansomwest_x_train, sansomwest_x_test, sansomwest_y_train, sansomwest_y_test = split_data(sansomwest_normalized)
johnsonpavilion_x_train, johnsonpavilion_x_test, johnsonpavilion_y_train, johnsonpavilion_y_test = split_data(johnsonpavilion_normalized)
museum_x_train, museum_x_test, museum_y_train, museum_y_test = split_data(museum_normalized)
singh_x_train, singh_x_test, singh_y_train, singh_y_test = split_data(singh_normalized)
vagelos_x_train, vagelos_x_test, vagelos_y_train, vagelos_y_test = split_data(vagelos_normalized)

# Regression metrics idea ripped from: https://stackoverflow.com/questions/26319259/how-to-get-a-regression-summary-in-python-scikit-like-r-does
from sklearn.linear_model import LinearRegression
import sklearn.metrics as metrics
def threeRegressionModels(x_train, x_test, y_train, y_test):
  regr1 = LinearRegression()
  regr1.fit(x_train, y_train['ELC'])
  y_pred1 = regr1.predict(x_test)

  explained_variance=metrics.explained_variance_score(y_test['ELC'], y_pred1)
  mean_absolute_error=metrics.mean_absolute_error(y_test['ELC'], y_pred1) 
  mse=metrics.mean_squared_error(y_test['ELC'], y_pred1)
  median_absolute_error=metrics.median_absolute_error(y_test['ELC'], y_pred1)
  r2=metrics.r2_score(y_test['ELC'], y_pred1)
  print("ELECTRICITY MODEL:\n")
  print('explained_variance: ', round(explained_variance,4))    
  print('r2: ', round(r2,4))
  print('MAE: ', round(mean_absolute_error,4))
  print('MSE: ', round(mse,4))
  print('RMSE: ', round(np.sqrt(mse),4))


  regr2 = LinearRegression()
  regr2.fit(x_train, y_train['CHW'])
  y_pred2 = regr2.predict(x_test)

  explained_variance=metrics.explained_variance_score(y_test['CHW'], y_pred2)
  mean_absolute_error=metrics.mean_absolute_error(y_test['CHW'], y_pred2) 
  mse=metrics.mean_squared_error(y_test['CHW'], y_pred2)
  median_absolute_error=metrics.median_absolute_error(y_test['CHW'], y_pred2)
  r2=metrics.r2_score(y_test['CHW'], y_pred2)
  print("\n\nCHILLED WATER MODEL:\n")
  print('explained_variance: ', round(explained_variance,4))    
  print('r2: ', round(r2,4))
  print('MAE: ', round(mean_absolute_error,4))
  print('MSE: ', round(mse,4))
  print('RMSE: ', round(np.sqrt(mse),4))


  regr3 = LinearRegression()
  regr3.fit(x_train, y_train['STM'])
  y_pred3 = regr3.predict(x_test)

  explained_variance=metrics.explained_variance_score(y_test['STM'], y_pred3)
  mean_absolute_error=metrics.mean_absolute_error(y_test['STM'], y_pred3) 
  mse=metrics.mean_squared_error(y_test['STM'], y_pred3)
  median_absolute_error=metrics.median_absolute_error(y_test['STM'], y_pred3)
  r2=metrics.r2_score(y_test['STM'], y_pred3)
  print("\n\nSTEAM MODEL:\n")
  print('explained_variance: ', round(explained_variance,4))    
  print('r2: ', round(r2,4))
  print('MAE: ', round(mean_absolute_error,4))
  print('MSE: ', round(mse,4))
  print('RMSE: ', round(np.sqrt(mse),4))

threeRegressionModels(annenberg_x_train, annenberg_x_test, annenberg_y_train, annenberg_y_test)

threeRegressionModels(blockley_x_train, blockley_x_test, blockley_y_train, blockley_y_test)

threeRegressionModels(brb1_x_train, brb1_x_test, brb1_y_train, brb1_y_test)

threeRegressionModels(addams_x_train, addams_x_test, addams_y_train, addams_y_test)

threeRegressionModels(fisherfine_x_train, fisherfine_x_test, fisherfine_y_train, fisherfine_y_test)

threeRegressionModels(sansomwest_x_train, sansomwest_x_test, sansomwest_y_train, sansomwest_y_test)

threeRegressionModels(johnsonpavilion_x_train, johnsonpavilion_x_test, johnsonpavilion_y_train, johnsonpavilion_y_test)

threeRegressionModels(museum_x_train, museum_x_test, museum_y_train, museum_y_test)

threeRegressionModels(singh_x_train, singh_x_test, singh_y_train, singh_y_test)

threeRegressionModels(vagelos_x_train, vagelos_x_test, vagelos_y_train, vagelos_y_test)

"""We can see the regression models are not very good, unfortunately.

A potential way to fix this problem is through PCA! PCA or principal component analysis, is a dimensionality-reduction method that allows us to limit multi-collinearity in our data and pick the features that explain the most variance. This might be really helpful to us if the weather data is highly correlated.

### PCA
From here, we want to figure out which features are best for us to use on our models. This can be done by using PCA, which analyzes our features and projects them in such a way that they can cover as much of the variance in the data as possible. This is great for limiting multi-collinearity and getting rid of variables that have no impact on the output. We will also plot the explained variance ratio vs. the number of components. From there, we can figure out which features will be used in our model.

First, we will define a quick function that'll perform PCA and plot the explained variance ratio:
"""

import numpy as np
from sklearn.decomposition import PCA

#Explained variance ratio plot
def modelPCA(trainx, trainy):
  pcaModel = PCA().fit(trainx, trainy)

  pc_vs_variance = np.cumsum(pcaModel.explained_variance_ratio_)

  pc_vs_variance
  plt.plot(pc_vs_variance)
  plt.plot([0, 6], [.95, .95])

  pca = PCA(n_components=3)
  pca.fit(trainx)
# and transform it
  return pca.transform(trainx)

"""Now, we will look at the covariance plots for our training data to see if PCA is really necessary. Since all of them use the same weather training data due to the buildings being in the same location, one correlation plot should be sufficient:"""

annenberg_x_train.corr()

"""In a peculiar twist of events, it seems like PCA actually isn't all that necessary! There isn't enough correlation between our features for it to be useful since most of them are <.3 (source: https://www.originlab.com/doc/Origin-Help/PrincipleComp-Analysis#:~:text=PCA%20should%20be%20used%20mainly,0.3%2C%20PCA%20will%20not%20help.), and it may actually end up hurting our model. A sample explained variance ratio graph is given below for comprehensiveness' sake:"""

annenberg_train_2 = modelPCA(annenberg_x_train, annenberg_y_train)

"""### Random Forest Model
Now that we've gotten our features set, we can try to do a more ML-esque model. A random forest should work well for this problem because there are a number of different features at play and a lot of different decision tree routes that the model could go down, so averaging them out can give us a good answer.

First, we tune the forest parameters with gridSearchCV, and then write a function that will run a random forest for each output variable.
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
#from tqdm import tqdm
def tunedForestParameters(x_train_2, y_train):
  param_grid = {
  'max_depth': [3, 4, 6, 7],
  'n_estimators': [20, 30, 50, 80, 100]
  }
  base_estimator = RandomForestRegressor(random_state=0)
  GS_object1 = GridSearchCV(base_estimator, param_grid).fit(x_train_2, y_train['ELC'])
  GS_object2 = GridSearchCV(base_estimator, param_grid).fit(x_train_2, y_train['CHW'])
  GS_object3 = GridSearchCV(base_estimator, param_grid).fit(x_train_2, y_train['STM'])
  return GS_object1, GS_object2, GS_object3

# feature importance chart code from: https://stackoverflow.com/questions/44101458/random-forest-feature-importance-chart-using-python
def threeRandomForests(GS_object1, GS_object2, GS_object3, x_test, y_test):

  depth = list(GS_object1.best_params_.values())[0]
  estimators = list(GS_object1.best_params_.values())[1]
  rfModel1 = RandomForestRegressor(n_estimators = estimators, max_depth = depth, random_state = 23)
  rfModel1.fit(x_test, y_test['ELC'])
  y_pred = rfModel1.predict(x_test)
  error = abs(y_pred - y_test['ELC'])
  mse_test = metrics.mean_squared_error(y_test['ELC'], y_pred)
  print("ELECTRICITY MODEL MSE: ", round(mse_test, 4))
  print("ELECTRICITY MODEL RMSE: ", round(np.sqrt(mse_test), 4))
  print("ELECTRICITY MODEL MAPE: ", round(np.mean(error), 4),)
  print("ELECTRICITY MODEL R^2: ", round(rfModel1.score(x_test, y_test['ELC']), 4), "\n\n")
  elc_importances = pd.Series(rfModel1.feature_importances_, index=x_test.columns)
  elc_importances.plot(kind='barh')
  plt.show()

  depth = list(GS_object2.best_params_.values())[0]
  estimators = list(GS_object2.best_params_.values())[1]
  rfModel2 = RandomForestRegressor(n_estimators = estimators, max_depth = depth, random_state = 23)
  rfModel2.fit(x_test, y_test['CHW'])
  y_pred = rfModel2.predict(x_test)
  error = abs(y_pred - y_test['CHW'])
  mse_test = metrics.mean_squared_error(y_test['CHW'], y_pred)
  print("\n\nCHILLED WATER MODEL MSE: ", round(mse_test, 4))
  print("CHILLED WATER MODEL RMSE: ", round(np.sqrt(mse_test), 4))
  print("CHILLED WATER MODEL MAPE: ", round(np.mean(error), 4),)
  print("CHILLED WATER MODEL R^2: ", round(rfModel2.score(x_test, y_test['CHW']), 4), "\n\n")
  chw_importances = pd.Series(rfModel2.feature_importances_, index=x_test.columns)
  chw_importances.plot(kind='barh')
  plt.show()

  depth = list(GS_object3.best_params_.values())[0]
  estimators = list(GS_object3.best_params_.values())[1]
  rfModel3 = RandomForestRegressor(n_estimators = estimators, max_depth = depth, random_state = 23)
  rfModel3.fit(x_test, y_test['STM'])
  y_pred = rfModel3.predict(x_test)
  error = abs(y_pred - y_test['STM'])
  mse_test = metrics.mean_squared_error(y_test['STM'], y_pred)
  print("STEAM MODEL MSE: ", round(mse_test, 4))
  print("STEAM MODEL RMSE: ", round(np.sqrt(mse_test), 4))
  print("STEAM MODEL MAPE: ", round(np.mean(error), 4),)
  print("STEAM MODEL R^2: ", round(rfModel3.score(x_test, y_test['STM']), 4))
  stm_importances = pd.Series(rfModel3.feature_importances_, index=x_test.columns)
  stm_importances.plot(kind='barh')
  plt.show()
  return rfModel1, rfModel2, rfModel3

"""Since random forests are scale-invariant and we aren't using PCA, it would be best practice to use the unscaled data that only has the deseasonalization transformation applied. Thus, we can actually interpret the Mean Absolute Percentage Error as how far off the model is on average in its predictions."""

annenberg_x_train, annenberg_x_test, annenberg_y_train, annenberg_y_test = split_data(annenberg_df)
blockley_x_train, blockley_x_test, blockley_y_train, blockley_y_test = split_data(blockley_df)
brb1_x_train, brb1_x_test, brb1_y_train, brb1_y_test = split_data(brb1_df)
addams_x_train, addams_x_test, addams_y_train, addams_y_test = split_data(addams_df)
fisherfine_x_train, fisherfine_x_test, fisherfine_y_train, fisherfine_y_test = split_data(fisherfine_df)
sansomwest_x_train, sansomwest_x_test, sansomwest_y_train, sansomwest_y_test = split_data(sansomwest_df)
johnsonpavilion_x_train, johnsonpavilion_x_test, johnsonpavilion_y_train, johnsonpavilion_y_test = split_data(johnsonpavilion_df)
museum_x_train, museum_x_test, museum_y_train, museum_y_test = split_data(museum_df)
singh_x_train, singh_x_test, singh_y_train, singh_y_test = split_data(singh_df)
vagelos_x_train, vagelos_x_test, vagelos_y_train, vagelos_y_test = split_data(vagelos_df)

import time

gs1, gs2, gs3 = tunedForestParameters(annenberg_x_train, annenberg_y_train)
annenberg_rf_1, annenberg_rf_2, annenberg_rf_3 = threeRandomForests(gs1, gs2, gs3, annenberg_x_test, annenberg_y_test)

gs1, gs2, gs3 = tunedForestParameters(blockley_x_train, blockley_y_train)
blockley_rf_1, blockley_rf_2, blockley_rf_3 =  threeRandomForests(gs1, gs2, gs3, blockley_x_test, blockley_y_test)

gs1, gs2, gs3 = tunedForestParameters(brb1_x_train, brb1_y_train)
brb1_rf_1, brb1_rf_2, brb1_rf_3 =  threeRandomForests(gs1, gs2, gs3, brb1_x_test, brb1_y_test)

gs1, gs2, gs3 = tunedForestParameters(addams_x_train, addams_y_train)
addams_rf_1, addams_rf_2, addams_rf_3 =  threeRandomForests(gs1, gs2, gs3, addams_x_test, addams_y_test)

gs1, gs2, gs3 = tunedForestParameters(fisherfine_x_train, fisherfine_y_train)
fisherfine_rf_1, fisherfine_rf_2, fisherfine_rf_3 =  threeRandomForests(gs1, gs2, gs3, fisherfine_x_test, fisherfine_y_test)

gs1, gs2, gs3 = tunedForestParameters(sansomwest_x_train, sansomwest_y_train)
sansomwest_rf_1, sansomwest_rf_2, sansomwest_rf_3 =  threeRandomForests(gs1, gs2, gs3, sansomwest_x_test, sansomwest_y_test)

gs1, gs2, gs3 = tunedForestParameters(johnsonpavilion_x_train, johnsonpavilion_y_train)
johnsonpavilion_rf_1, johnsonpavilion_rf_2, johnsonpavilion_rf_3 =  threeRandomForests(gs1, gs2, gs3, johnsonpavilion_x_test, johnsonpavilion_y_test)

gs1, gs2, gs3 = tunedForestParameters(museum_x_train, museum_y_train)
museum_rf_1, museum_rf_2, museum_rf_3 =  threeRandomForests(gs1, gs2, gs3, museum_x_test, museum_y_test)

gs1, gs2, gs3 = tunedForestParameters(singh_x_train, singh_y_train)
singh_rf_1, singh_rf_2, singh_rf_3 = threeRandomForests(gs1, gs2, gs3, singh_x_test, singh_y_test)

gs1, gs2, gs3 = tunedForestParameters(vagelos_x_train, vagelos_y_train)
vagelos_rf_1, vagelos_rf_2, vagelos_rf_3 = threeRandomForests(gs1, gs2, gs3, vagelos_x_test, vagelos_y_test)

"""### Comparing the models and some conclusions

First, we can see that the R^2 and accuracy for the random forests are high. While random forests are a bit more black-boxed than a decision tree, we can still see the predictive power. Since this was not captured in the regression models, I believe that maybe energy usage should be modeled as a set of conditions, rather than directly correlated variables. For example, electricity usage may only peak when the outside temperature is above/below a certain level, rather than a linear relationship. This matches with the intuition that heating/cooling systems may be tripped off by external weather monitors. The university only turns on heating/cooling for dorm buildings once the average temperature reaches a certain level, for example.

Also, the MAPE (mean absolute percentage error) and MSE for all of these models is fairly low, even though the outputs are not scaled down. This implies that these forests are able to model the outputs with decent. An additional thought is that some outliers are hurting the residual values predicted by the model and thus dropping the R^2 while not affecting the MAPE as much, and the deseasonalized graph does show some large outliers to support that idea.

From the data, we see that the Johnson Pavilion and Museum steam models have a particularly high RMSE value. One conclusion we can draw is that it may be useful to have the steam gauges checked and/or to recommision the steam valves in these buildings. It is possible they are inefficient, faulty, or too old to function properly, which is leading to such an error in the model!

Overall, we have learned a lot from this model-building process and have demonstrated that weather data is useful for predicting energy usage in buildings across campus.
"""